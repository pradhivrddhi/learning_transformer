{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Tour of Transformer Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dear Amazon, last week I ordered an Optimius Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Dear Amazon, last week I ordered an Optimius Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\"\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines abstract away all the steps needed to convert raw text into a set of predictions from a fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task here is to extract the sentiment of an input text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time execution of the following loads the model weights and put them into cache. This prevents the following executions from calling these weights again, and thus the subsequent executions directly works without a progress bar.\n",
    "\n",
    "By default, the ``text-classification`` pipeline uses a model that's designed for sentiment analysis, but it also supports multiclass and multilabel classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('text-classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the default classifier pipeline on the global text string, yields a ``NEGATIVE`` label as can be interpreted from text showing negative sentiment. The score of 0.901+ suggests that sureity of this being a negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9014247059822083}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = classifier(text)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, only the ``NEGATIVE`` sentiment is returned because the score for the ``POSITIVE`` sentiment is just 1-above_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.901425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label     score\n",
       "0  NEGATIVE  0.901425"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task here is to identify the named entities in the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP, real-world objects like products, places, and people are called named entities, and extracting them from text is called ``named entity recognition (NER)``.\n",
    "\n",
    "The name of task given to pipeline thus becomes ``'ner'``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nitin\\.cache\\huggingface\\hub\\models--dbmdz--bert-large-cased-finetuned-conll03-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.token_classification.TokenClassificationPipeline at 0x1e8aca52e90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tagger = pipeline('ner', aggregation_strategy='simple')\n",
    "ner_tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input requried is the text on which named entities are to be discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'ORG',\n",
       "  'score': 0.8653713,\n",
       "  'word': 'Amazon',\n",
       "  'start': 5,\n",
       "  'end': 11},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.97458327,\n",
       "  'word': 'Optimius Prime',\n",
       "  'start': 36,\n",
       "  'end': 50},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99975747,\n",
       "  'word': 'Germany',\n",
       "  'start': 91,\n",
       "  'end': 98},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.49259824,\n",
       "  'word': 'Mega',\n",
       "  'start': 209,\n",
       "  'end': 213},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.59641314,\n",
       "  'word': '##tron',\n",
       "  'start': 213,\n",
       "  'end': 217},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.6183118,\n",
       "  'word': 'Decept',\n",
       "  'start': 254,\n",
       "  'end': 260},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.50714475,\n",
       "  'word': '##icons',\n",
       "  'start': 260,\n",
       "  'end': 265},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.74730206,\n",
       "  'word': 'Megatron',\n",
       "  'start': 351,\n",
       "  'end': 359},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.97488445,\n",
       "  'word': 'Optimus Prime',\n",
       "  'start': 368,\n",
       "  'end': 381},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.8137787,\n",
       "  'word': 'Bumblebee',\n",
       "  'start': 503,\n",
       "  'end': 512}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = ner_tagger(text)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.865371</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.974583</td>\n",
       "      <td>Optimius Prime</td>\n",
       "      <td>36</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.999757</td>\n",
       "      <td>Germany</td>\n",
       "      <td>91</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.492598</td>\n",
       "      <td>Mega</td>\n",
       "      <td>209</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.596413</td>\n",
       "      <td>##tron</td>\n",
       "      <td>213</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.618312</td>\n",
       "      <td>Decept</td>\n",
       "      <td>254</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.507145</td>\n",
       "      <td>##icons</td>\n",
       "      <td>260</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.747302</td>\n",
       "      <td>Megatron</td>\n",
       "      <td>351</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.974884</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>368</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.813779</td>\n",
       "      <td>Bumblebee</td>\n",
       "      <td>503</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score            word  start  end\n",
       "0          ORG  0.865371          Amazon      5   11\n",
       "1         MISC  0.974583  Optimius Prime     36   50\n",
       "2          LOC  0.999757         Germany     91   98\n",
       "3         MISC  0.492598            Mega    209  213\n",
       "4          PER  0.596413          ##tron    213  217\n",
       "5          ORG  0.618312          Decept    254  260\n",
       "6         MISC  0.507145         ##icons    260  265\n",
       "7         MISC  0.747302        Megatron    351  359\n",
       "8         MISC  0.974884   Optimus Prime    368  381\n",
       "9          PER  0.813779       Bumblebee    503  512"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_gpt_answer = \"\"\"\n",
    "Improving predictions made using predictive analytics models involves several strategies and considerations. Here are some key approaches:\n",
    "\n",
    "Data Quality and Quantity: Ensure that your data is clean, relevant, and sufficient for training the model. High-quality data leads to more accurate predictions. Additionally, having more data can help the model learn more complex patterns and relationships.\n",
    "\n",
    "Feature Selection and Engineering: Identify the most important features that contribute to the prediction task and focus on those. Feature engineering involves creating new features or transforming existing ones to better represent the underlying patterns in the data.\n",
    "\n",
    "Model Selection and Tuning: Experiment with different algorithms and model architectures to find the one that best fits your data and problem. Additionally, fine-tune hyperparameters to optimize the model's performance.\n",
    "\n",
    "Cross-Validation: Use techniques like k-fold cross-validation to assess the model's performance on different subsets of the data. This helps ensure that the model generalizes well to unseen data.\n",
    "\n",
    "Ensemble Methods: Combine predictions from multiple models to improve accuracy and robustness. Techniques like bagging, boosting, and stacking can help create more powerful predictive models.\n",
    "\n",
    "Regularization: Prevent overfitting by applying regularization techniques such as L1 and L2 regularization, dropout, or early stopping. These techniques help prevent the model from memorizing noise in the training data.\n",
    "\n",
    "Model Interpretability: Aim for models that are not only accurate but also interpretable. This allows stakeholders to understand why the model makes certain predictions, which can lead to more trust and easier implementation.\n",
    "\n",
    "Domain Knowledge Integration: Incorporate domain knowledge into the model-building process. Subject matter experts can provide valuable insights that may not be apparent from the data alone.\n",
    "\n",
    "Continuous Monitoring and Updating: Regularly monitor the model's performance in production and update it as needed. Data distributions may change over time, requiring the model to adapt to new patterns.\n",
    "\n",
    "Feedback Loop: Establish a feedback loop where predictions are evaluated against ground truth outcomes, and the model is continuously refined based on this feedback. This iterative process helps improve the model's accuracy over time.\n",
    "\n",
    "By following these strategies and continuously refining the predictive analytics process, you can improve the accuracy and reliability of your predictions.\n",
    "\"\"\".replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_gpt_answer_outputs = ner_tagger(chat_gpt_answer)\n",
    "pd.DataFrame(chat_gpt_answer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(chat_gpt_answer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_gpt_answer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_text = \"\"\"I am not getting much on the subject of Data Science. What can be the problem? Does it not recognize things like Predictive Analytics?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.617478</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>40</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.511884</td>\n",
       "      <td>Pre</td>\n",
       "      <td>113</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.673870</td>\n",
       "      <td>Analytics</td>\n",
       "      <td>124</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score          word  start  end\n",
       "0          ORG  0.617478  Data Science     40   52\n",
       "1         MISC  0.511884           Pre    113  116\n",
       "2         MISC  0.673870     Analytics    124  133"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alternative_text_outputs = ner_tagger(alternative_text)\n",
    "pd.DataFrame(alternative_text_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_definition = \"\"\"BERT stands for “Bidirectional Encoder Representation with Transformers”. To put it in simple words BERT extracts patterns or representations from the data or word embeddings by passing it through an encoder. The encoder itself is a transformer architecture that is stacked together. It is a bidirectional transformer which means that during training it considers the context from both left and right of the vocabulary to extract patterns or representations. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.599849</td>\n",
       "      <td>BER</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.360904</td>\n",
       "      <td>Transformers</td>\n",
       "      <td>59</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.916962</td>\n",
       "      <td>BER</td>\n",
       "      <td>100</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score          word  start  end\n",
       "0          ORG  0.599849           BER      0    3\n",
       "1          ORG  0.360904  Transformers     59   71\n",
       "2          ORG  0.916962           BER    100  103"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_definition_outputs = ner_tagger(bert_definition)\n",
    "pd.DataFrame(bert_definition_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I observed can be listed as follows:\n",
    "1. The output for the original text was amazing and very helpful as well as usable for such a task.\n",
    "2. This failed to work on large texts as well as examples having strange(?) named entities.\n",
    "3. This model is trained on ORG(organization), LOC (location), or PER (person). So, to make it work on named entities from a different system a second round of training on those systems will be required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Anaswering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task here is to get answers to direct questions from a text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In question answering, we provide the model with a passage of text called the context, along with a question whose answer we'd like to extract. The model then returns a span of text corresponding to the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nitin\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.question_answering.QuestionAnsweringPipeline at 0x1e8aeca07d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = pipeline('question-answering')\n",
    "reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'What does the customer want?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.590901255607605,\n",
       " 'start': 336,\n",
       " 'end': 359,\n",
       " 'answer': 'an exchange of Megatron'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = reader(question=question, context=text)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.590901</td>\n",
       "      <td>336</td>\n",
       "      <td>359</td>\n",
       "      <td>an exchange of Megatron</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  start  end                   answer\n",
       "0  0.590901    336  359  an exchange of Megatron"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Improving predictions made using predictive analytics models involves several strategies and considerations. Here are some key approaches:Data Quality and Quantity: Ensure that your data is clean, relevant, and sufficient for training the model. High-quality data leads to more accurate predictions. Additionally, having more data can help the model learn more complex patterns and relationships.Feature Selection and Engineering: Identify the most important features that contribute to the prediction task and focus on those. Feature engineering involves creating new features or transforming existing ones to better represent the underlying patterns in the data.Model Selection and Tuning: Experiment with different algorithms and model architectures to find the one that best fits your data and problem. Additionally, fine-tune hyperparameters to optimize the model's performance.Cross-Validation: Use techniques like k-fold cross-validation to assess the model's performance on different subsets of the data. This helps ensure that the model generalizes well to unseen data.Ensemble Methods: Combine predictions from multiple models to improve accuracy and robustness. Techniques like bagging, boosting, and stacking can help create more powerful predictive models.Regularization: Prevent overfitting by applying regularization techniques such as L1 and L2 regularization, dropout, or early stopping. These techniques help prevent the model from memorizing noise in the training data.Model Interpretability: Aim for models that are not only accurate but also interpretable. This allows stakeholders to understand why the model makes certain predictions, which can lead to more trust and easier implementation.Domain Knowledge Integration: Incorporate domain knowledge into the model-building process. Subject matter experts can provide valuable insights that may not be apparent from the data alone.Continuous Monitoring and Updating: Regularly monitor the model's performance in production and update it as needed. Data distributions may change over time, requiring the model to adapt to new patterns.Feedback Loop: Establish a feedback loop where predictions are evaluated against ground truth outcomes, and the model is continuously refined based on this feedback. This iterative process helps improve the model's accuracy over time.By following these strategies and continuously refining the predictive analytics process, you can improve the accuracy and reliability of your predictions.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_gpt_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'How can feature selection improve predictions?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.6316820383071899,\n",
       " 'start': 2340,\n",
       " 'end': 2428,\n",
       " 'answer': 'By following these strategies and continuously refining the predictive analytics process'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_gpt_outputs = reader(question=question, context=chat_gpt_answer)\n",
    "chat_gpt_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.631682</td>\n",
       "      <td>2340</td>\n",
       "      <td>2428</td>\n",
       "      <td>By following these strategies and continuously...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  start   end                                             answer\n",
       "0  0.631682   2340  2428  By following these strategies and continuously..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([chat_gpt_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BERT stands for “Bidirectional Encoder Representation with Transformers”. To put it in simple words BERT extracts patterns or representations from the data or word embeddings by passing it through an encoder. The encoder itself is a transformer architecture that is stacked together. It is a bidirectional transformer which means that during training it considers the context from both left and right of the vocabulary to extract patterns or representations. '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_question = 'What is unique about BERT?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.07160419970750809,\n",
       " 'start': 105,\n",
       " 'end': 174,\n",
       " 'answer': 'extracts patterns or representations from the data or word embeddings'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_answer = reader(question=bert_question, context=bert_definition)\n",
    "bert_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following conclusions can be drawn from this:\n",
    "\n",
    "1. The default model works well with the generate language questions and small queries on a small context.\n",
    "\n",
    "2. The reader above returns the start and end of the part from where answer was extracted.\n",
    "\n",
    "3. The answer appears as an extracted text rather than a well formed English sentence.\n",
    "\n",
    "4. The efficiency of the algorithm decreases as context becomes larger or the answer becomes more spread out within the context.\n",
    "\n",
    "5. This kind of question-answering is what the authors are calling the **extractive question answering** because the answer is extracted directly from the text.\n",
    "\n",
    "6. This is still helpful if a uniform question is to be asked from a plethora of contexts having similar answers as is the case with the original text. This is so because the answers here are helpful in finding the right department to address the issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of text summarization is to take a long text as input and generate a short version with all the relevant facts. This requires the model to generate coherent text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summarization pipeline can be defined as follows and uses the token ``summarization`` as parameter as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nitin\\.cache\\huggingface\\hub\\models--sshleifer--distilbart-cnn-12-6. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline('summarization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement the summarizer on the original test first and then the chatgpt answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your min_length=56 must be inferior than your max_length=45.\n",
      "c:\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1156: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (45). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' Bumblebee demands an exchange of Megatron for the Optimus Prime figure he ordered. The Decepticons are a lifelong enemy of the Decepticon, BumbleBee writes to Amazon.com.com.'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = summarizer(text, max_length=45, clean_up_tokenization_spaces=True)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictive Analytics Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 500, but your input_length is only 445. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=222)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' High-quality data leads to more accurate predictions, and more data can help the model learn more complex patterns and relationships. Feature engineering involves creating new features or transforming existing ones to better represent the underlying patterns in the data. Experiment with different algorithms and model architectures to find the one that best fits your data and problem.'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = summarizer(chat_gpt_answer, max_length=500, clean_up_tokenization_spaces=True)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following conclusions can be drawn from this:\n",
    "\n",
    "1. There are stray words that can be avoided from the summerization but the important points are quite clear. This default model seems easy to use and small enough for a normal app to download.\n",
    "\n",
    "2. The size and scope of summary seems to be limited. But quality inputs is infact leading to quality output.\n",
    "\n",
    "3. Various named entities within the small scope were well identified and were well placed in summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translation is a task where the output consists of generated text which is the required translation from the laguage of the original text to the required language in which output is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task of a translator requires the mention of original as well as the target language. Here we are going to mention a model as well as the languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nitin\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-de. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\anaconda3\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "translator = pipeline('translation_en_to_de', model='Helsinki-NLP/opus-mt-en-de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: regex in c:\\anaconda3\\lib\\site-packages (from sacremoses) (2023.12.25)\n",
      "Requirement already satisfied: click in c:\\anaconda3\\lib\\site-packages (from sacremoses) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\anaconda3\\lib\\site-packages (from sacremoses) (1.3.2)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda3\\lib\\site-packages (from sacremoses) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\anaconda3\\lib\\site-packages (from click->sacremoses) (0.4.6)\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "   ---------------------------------------- 0.0/897.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/897.5 kB ? eta -:--:--\n",
      "   - ------------------------------------- 30.7/897.5 kB 435.7 kB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 112.6/897.5 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 245.8/897.5 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 368.6/897.5 kB 1.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 491.5/897.5 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 573.4/897.5 kB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 624.6/897.5 kB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 696.3/897.5 kB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 727.0/897.5 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  890.9/897.5 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 897.5/897.5 kB 1.9 MB/s eta 0:00:00\n",
      "Installing collected packages: sacremoses\n",
      "Successfully installed sacremoses-0.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script sacremoses.exe is installed in 'C:\\Users\\nitin\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This translator requires the text to be translated and among other things one can give instructions like cleaning up of tokenization spaces or min_length as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Sehr geehrter Amazon, letzte Woche habe ich eine Optimius Prime Action Figur von Ihrem Online-Shop in Deutschland bestellt. Leider, als ich das Paket öffnete, entdeckte ich zu meinem Entsetzen, dass ich stattdessen eine Action Figur von Megatron geschickt worden war! Als lebenslanger Feind der Decepticons, Ich hoffe, dass Sie mein Dilemma verstehen können. Um das Problem zu lösen, fordere ich einen Austausch von Megatron für die Optimus Prime Figur, die ich bestellt hatte.'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a good enough translation and should be as stated by the auther. Not being a speaker myself, I am not able to comment. So, I will try to get one for hindi language instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One in Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nitin\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-hi. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "hindi_translator = pipeline('translation_en_to_hi', model='Helsinki-NLP/opus-mt-en-hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'प्रिय एमिला, पिछले हफ्ते मैंने जर्मनी में आपके ऑनलाइन स्टोर से एक ऑप्टिमिमिमीय कार्रवाई की घोषणा की. दुःख की बात है, जब मैंने पैकेज खोला था, तब मुझे लगा कि मैं मेगॉरॉन का एक कार्य आकृति भेजा गया था! इसके बजाय, मैं आशा करता हूँ कि आप मेरी दुविधा को समझ सकते हैं. मुझे लगता है कि आप मेरी दुविधाओं को बदलने की मांग कर रहे हैं. मुझे इस तस्वीर के बारे में जो मैंने कहा था, उसकी बहुत जल्द आप सुन सकते हैं.'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = hindi_translator(text, clean_up_tokenization_spaces=True, min_length=100)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might just be hindi translations and lack of training data. But this is horrible translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nitin\\.cache\\huggingface\\hub\\models--Karn07--engilsh_to_hindi_translation. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "hindi_translator = pipeline('translation_en_to_hi', model='Karn07/engilsh_to_hindi_translation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 121 is bigger than 0.9 * max_length: 20. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "c:\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1156: UserWarning: Unfeasible length constraints: `min_length` (4000) is larger than the maximum possible length (20). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length. Note that `max_length` is set to 20, its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': ',         '}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = hindi_translator(text, clean_up_tokenization_spaces=True, min_length=4000)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This alternate model just refuses to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are a few of my observation:\n",
    "\n",
    "1. Not knowing German, I cannot comment on the quality of the output from the book example.\n",
    "\n",
    "2. In Hindi translation, there is quite a lot left to desire, but I can imagine a worse scenario. This can be attributed to a general lack of availabilty of Hindi text and English to Hindi translation on the topic. I also believe that the low quality is also due to the fact that pretrained Hindi models are practically non-existant.\n",
    "\n",
    "3. This situation should be worse for even rarer languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Generation lets the user produce an autocomplete function that generates more text in the continuation of what is provided earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assigning Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator is built using the parameter ``'text-generation'`` to the ``pipeline`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nitin\\.cache\\huggingface\\hub\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we end up loading gpt2 which we are already aware of for its quality, still a prompt is required to be constructed to achieve any result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "response = 'Dear Bumblebee, I am sorry to hear that your order was mixed up.'\n",
    "prompt = text + '\\n\\nCustomer service response:\\n' + response\n",
    "outputs = generator(prompt, max_length=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
